{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO9fe/36hTAgEzeUOqS2OsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeAreDobby/CV_project/blob/main/AutoEncoder/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Google Drive 연동\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEdz7I1vF8Er",
        "outputId": "c4ba1f4f-a2e6-49fa-d59b-3f097844faaf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from IPython import display\r\n",
        "\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "import glob\r\n",
        "import imageio\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_probability as tfp\r\n",
        "import time"
      ],
      "outputs": [],
      "metadata": {
        "id": "TJU4_mO3CNdC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Hyper Parameter\r\n",
        "EPOCHS = 10000\r\n",
        "channel = 1\r\n",
        "image_size = 128\r\n",
        "latent_size = 100  #latent vector size\r\n",
        "batch_size = 64\r\n",
        "learning_rate = 0.0001\r\n",
        "read_type = lambda channel: 'rgb' if channel==3 else 'gray'"
      ],
      "outputs": [],
      "metadata": {
        "id": "3x4JfTutFCMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Load Pocketmon Dataset\r\n",
        "def find_path(path):\r\n",
        "    file_list = os.listdir(path)\r\n",
        "    return file_list\r\n",
        "\r\n",
        "def png_to_np(path, file_list, read_type='rgb'):\r\n",
        "    pix = []\r\n",
        "    for idx in tqdm(range(len(file_list))):\r\n",
        "      file_name = file_list[idx]\r\n",
        "      if read_type == \"rgb\":\r\n",
        "        image = PIL.Image.open(path + '/' + file_name) # (256, 256, 3)\r\n",
        "      elif read_type == \"gray\":\r\n",
        "        image = PIL.Image.open(path + '/' + file_name).convert(\"L\") # (256, 256, 1)\r\n",
        "      np_image = (np.array(image).astype(np.float32) - 127.5) / 127.5\r\n",
        "      pix.append(np_image)\r\n",
        "    return pix"
      ],
      "outputs": [],
      "metadata": {
        "id": "_xw66z7FFq_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "path = 'drive/MyDrive/dataset/Human/Human_{}'.format(str(image_size))  # 'drive/MyDrive/dataset/Dobby/Dobby_{}' #'drive/MyDrive/dataset/pockemon/jpg/jpg_dataset_{}'\r\n",
        "\r\n",
        "file_list = find_path(path)\r\n",
        "images = png_to_np(path, file_list, read_type=read_type(channel))\r\n",
        "train_dataset = np.array(images)\r\n",
        "\r\n",
        "# GrayScale일때 1channel을 만들어주기위해 활용\r\n",
        "if channel == 1:\r\n",
        "  train_dataset = train_dataset[..., tf.newaxis]\r\n",
        "  \r\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_dataset)\r\n",
        "                 .shuffle(30).batch(batch_size))\r\n",
        "\r\n",
        "train_dataset"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 770/770 [03:08<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (None, 128, 128, 1), types: tf.float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rlPTFjeF4Vz",
        "outputId": "1b44cd9c-88be-4ca2-bd56-7d6f32a4226f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Generator\r\n",
        "class Generator(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Generator, self).__init__()\r\n",
        "    dense = tf.keras.layers.Dense\r\n",
        "    batchNormalization = tf.keras.layers.BatchNormalization\r\n",
        "    reshape = tf.keras.layers.Reshape\r\n",
        "    leakyrelu = tf.keras.layers.LeakyReLU()\r\n",
        "\r\n",
        "    self.sequence = list()\r\n",
        "    # Block 1\r\n",
        "    self.sequence.append(dense(128, activation=leakyrelu))\r\n",
        "    self.sequence.append(batchNormalization())\r\n",
        "    # Block 2\r\n",
        "    self.sequence.append(dense(512, activation=leakyrelu))\r\n",
        "    self.sequence.append(batchNormalization())\r\n",
        "    # Block 3\r\n",
        "    self.sequence.append(dense(2048, activation=leakyrelu))\r\n",
        "    self.sequence.append(batchNormalization())\r\n",
        "    # Block 4\r\n",
        "    self.sequence.append(dense(image_size**2, activation=leakyrelu))\r\n",
        "    self.sequence.append(batchNormalization())\r\n",
        "    \r\n",
        "    self.sequence.append(reshape([image_size, image_size, 1]))\r\n",
        "    \r\n",
        "  def call(self, z):\r\n",
        "    for layer in self.sequence:\r\n",
        "      z = layer(z)\r\n",
        "\r\n",
        "    return z"
      ],
      "outputs": [],
      "metadata": {
        "id": "NQbWSuIGuDDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# Discriminator\r\n",
        "class Discriminator(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Discriminator, self).__init__()\r\n",
        "    conv2d = tf.keras.layers.Conv2D\r\n",
        "    maxpool = tf.keras.layers.MaxPool2D\r\n",
        "    dense = tf.keras.layers.Dense\r\n",
        "    flatten = tf.keras.layers.Flatten\r\n",
        "    leakyrelu = tf.keras.layers.LeakyReLU()\r\n",
        "\r\n",
        "    self.sequence = list()\r\n",
        "    # Block 1\r\n",
        "    self.sequence.append(conv2d(8, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))  \r\n",
        "    self.sequence.append(maxpool((2, 2)))                                      \r\n",
        "    # Block 2\r\n",
        "    self.sequence.append(conv2d(32, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))\r\n",
        "    self.sequence.append(maxpool((2, 2)))\r\n",
        "    # Block 3\r\n",
        "    self.sequence.append(conv2d(64, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))\r\n",
        "    self.sequence.append(maxpool((2, 2)))\r\n",
        "    # Block 4\r\n",
        "    self.sequence.append(conv2d(128, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))\r\n",
        "    self.sequence.append(flatten())\r\n",
        "    # Block 5\r\n",
        "    self.sequence.append(dense(1024, activation=leakyrelu))\r\n",
        "    self.sequence.append(dense(1))\r\n",
        "    \r\n",
        "  def call(self, x):\r\n",
        "    for layer in self.sequence:\r\n",
        "      x = layer(x)\r\n",
        "\r\n",
        "    return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "_1iL5aT4uB9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Noise 생성부\r\n",
        "def make_Noise(batch_size, latent_size):\r\n",
        "  return tf.random.normal((batch_size, latent_size))\r\n",
        "\r\n",
        "# D_Model Loss 연산\r\n",
        "def discriminator_loss(loss, d_result_real, d_result_fake):\r\n",
        "    real_loss = loss(tf.ones_like(d_result_real), d_result_real)\r\n",
        "    fake_loss = loss(tf.zeros_like(d_result_fake), d_result_fake)\r\n",
        "    total_loss = real_loss + fake_loss\r\n",
        "    return total_loss\r\n",
        "\r\n",
        "# D_Model Optimizer\r\n",
        "def discriminator_optimizer(learning_rate):\r\n",
        "    return tf.keras.optimizers.Adam(learning_rate)\r\n",
        "\r\n",
        "# G_Model Loss 연산\r\n",
        "def generator_loss(loss, d_result_fake):\r\n",
        "    return loss(tf.ones_like(d_result_fake), d_result_fake)\r\n",
        "\r\n",
        "# G_Model Optimizer\r\n",
        "def generator_optimizer(learning_rate):\r\n",
        "    return tf.keras.optimizers.Adam(learning_rate)\r\n",
        "\r\n",
        "# G_Model에서 생성된 이미지 저장하는거\r\n",
        "def generate_and_save_images(g_model, epoch):\r\n",
        "  noises = make_Noise(32, 100)\r\n",
        "  g_predict = g_model(noises, training=False)\r\n",
        "\r\n",
        "  fig = plt.figure(figsize=(6,6))\r\n",
        "\r\n",
        "  for i in range(g_predict.shape[0]):\r\n",
        "      plt.subplot(6, 6, i+1)\r\n",
        "      plt.imshow(g_predict[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\r\n",
        "      plt.axis('off')\r\n",
        "\r\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(g_model, d_model, loss_function, g_optimizer, d_optimizer, batch_size, latent_size, learning_rate, real_images):\r\n",
        "  noises = make_Noise(batch_size, latent_size)\r\n",
        "\r\n",
        "  with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\r\n",
        "    g_images = g_model(noises)\r\n",
        "\r\n",
        "    d_result_real = d_model(real_images, training=True)\r\n",
        "    d_result_fake = d_model(g_images, training=True)\r\n",
        "\r\n",
        "    g_loss = generator_loss(loss_function, d_result_fake)\r\n",
        "    d_loss = discriminator_loss(loss_function, d_result_real, d_result_fake)\r\n",
        "\r\n",
        "  g_gradients = g_tape.gradient(g_loss, g_model.trainable_variables)\r\n",
        "  d_gradients = d_tape.gradient(d_loss, d_model.trainable_variables)\r\n",
        "\r\n",
        "  g_optimizer.apply_gradients(zip(g_gradients, g_model.trainable_variables))\r\n",
        "  d_optimizer.apply_gradients(zip(d_gradients, d_model.trainable_variables))"
      ],
      "outputs": [],
      "metadata": {
        "id": "UBVYtTCMTDSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "g_model = Generator()\r\n",
        "d_model = Discriminator()\r\n",
        "\r\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
        "g_optimizer = generator_optimizer(learning_rate)\r\n",
        "d_optimizer = discriminator_optimizer(learning_rate)\r\n",
        "\r\n",
        "for epoch in tqdm(range(EPOCHS)):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  for real_images in train_dataset:\r\n",
        "    train_step(g_model, d_model, loss_function, g_optimizer, d_optimizer, batch_size, latent_size, learning_rate, real_images)\r\n",
        "\r\n",
        "  if (epoch % 100) == 0:\r\n",
        "    generate_and_save_images(g_model, epoch)\r\n",
        "\r\n",
        "  # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\r\n",
        "  # print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 4576/10000 [33:08<39:16,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7722896ebe9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mreal_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m       dataset = _OptimizeDataset(dataset, graph_rewrites.enabled,\n\u001b[1;32m    462\u001b[0m                                  \u001b[0mgraph_rewrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                                  graph_rewrites.default, graph_rewrite_configs)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;31m# (4) Apply stats aggregator options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, optimization_configs)\u001b[0m\n\u001b[1;32m   4894\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4895\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4896\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4898\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset_v2\u001b[0;34m(input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0moptimizations_disabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   4175\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "BpiYT3ovWM_5",
        "outputId": "fdf87855-f9d6-42ac-fdd0-fc346ba40c0e"
      }
    }
  ]
}