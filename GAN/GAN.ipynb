{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO9fe/36hTAgEzeUOqS2OsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeAreDobby/CV_project/blob/main/AutoEncoder/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEdz7I1vF8Er",
        "outputId": "c4ba1f4f-a2e6-49fa-d59b-3f097844faaf"
      },
      "source": [
        "# Google Drive 연동\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJU4_mO3CNdC"
      },
      "source": [
        "from IPython import display\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x4JfTutFCMb"
      },
      "source": [
        "# Hyper Parameter\n",
        "EPOCHS = 10000\n",
        "channel = 1\n",
        "image_size = 128\n",
        "latent_size = 100  #latent vector size\n",
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "read_type = lambda channel: 'rgb' if channel==3 else 'gray'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xw66z7FFq_z"
      },
      "source": [
        "# Load Pocketmon Dataset\n",
        "def find_path(path):\n",
        "    file_list = os.listdir(path)\n",
        "    return file_list\n",
        "\n",
        "def png_to_np(path, file_list, read_type='rgb'):\n",
        "    pix = []\n",
        "    for idx in tqdm(range(len(file_list))):\n",
        "      file_name = file_list[idx]\n",
        "      if read_type == \"rgb\":\n",
        "        image = PIL.Image.open(path + '/' + file_name) # (256, 256, 3)\n",
        "      elif read_type == \"gray\":\n",
        "        image = PIL.Image.open(path + '/' + file_name).convert(\"L\") # (256, 256, 1)\n",
        "      np_image = (np.array(image).astype(np.float32) - 127.5) / 127.5\n",
        "      pix.append(np_image)\n",
        "    return pix"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rlPTFjeF4Vz",
        "outputId": "1b44cd9c-88be-4ca2-bd56-7d6f32a4226f"
      },
      "source": [
        "path = 'drive/MyDrive/dataset/Human/Human_{}'.format(str(image_size))  # 'drive/MyDrive/dataset/Dobby/Dobby_{}' #'drive/MyDrive/dataset/pockemon/jpg/jpg_dataset_{}'\n",
        "\n",
        "file_list = find_path(path)\n",
        "images = png_to_np(path, file_list, read_type=read_type(channel))\n",
        "train_dataset = np.array(images)\n",
        "\n",
        "# GrayScale일때 1channel을 만들어주기위해 활용\n",
        "if channel == 1:\n",
        "  train_dataset = train_dataset[..., tf.newaxis]\n",
        "  \n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_dataset)\n",
        "                 .shuffle(30).batch(batch_size))\n",
        "\n",
        "train_dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 770/770 [03:08<00:00,  4.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (None, 128, 128, 1), types: tf.float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPvgjRAHcpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "43e33af9-792c-4539-c0d1-9b15dd6af5f5"
      },
      "source": [
        "\"\"\"inputshape = tf.keras.layers.InputLayer\n",
        "dense = tf.keras.layers.Dense\n",
        "conv2d = tf.keras.layers.Conv2D\n",
        "maxpool = tf.keras.layers.MaxPool2D\n",
        "flatten = tf.keras.layers.Flatten\n",
        "reshape = tf.keras.layers.Reshape\n",
        "batchNormalization = tf.keras.layers.BatchNormalization\n",
        "leakyrelu = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "def generator():\n",
        "  model = tf.keras.Sequential(name='Generator')\n",
        "\n",
        "  model.add(inputshape((latent_size, )))\n",
        "  model.add(dense(128, activation=leakyrelu))\n",
        "  model.add(batchNormalization())\n",
        "    \n",
        "  model.add(dense(512, activation=leakyrelu))\n",
        "  model.add(batchNormalization())\n",
        "      \n",
        "  model.add(dense(2048, activation=leakyrelu))\n",
        "  model.add(batchNormalization())\n",
        "      \n",
        "  model.add(dense(image_size**2, activation=leakyrelu))\n",
        "  model.add(batchNormalization())\n",
        "      \n",
        "  model.add(reshape((image_size, image_size, 1)))\n",
        "\n",
        "  g_model = tf.keras.Model(\n",
        "    inputs=model.get_layer(index=0),\n",
        "    outputs=model.get_layer(index=-1),\n",
        "  )\n",
        "\n",
        "  return g_model\n",
        "\n",
        "def discriminator():\n",
        "  model = tf.keras.Sequential(name='Discriminator')\n",
        "\n",
        "  model.add(inputshape((image_size, image_size, 1)))\n",
        "  model.add()\n",
        "  model.add(maxpool((2, 2)))\n",
        "\n",
        "  model.add()\n",
        "  model.add(maxpool((2, 2)))\n",
        "\n",
        "  model.add()\n",
        "  model.add(maxpool((2, 2)))\n",
        "\n",
        "  model.add()\n",
        "  model.add(flatten())\n",
        "\n",
        "  model.add(dense(1024, activation=leakyrelu))\n",
        "  model.add(dense(1))\n",
        "\n",
        "  d_model = tf.keras.Model(\n",
        "    inputs=model.get_layer(index=0),\n",
        "    outputs=model.get_layer(index=-1),\n",
        "  )\n",
        "\n",
        "  return d_model\"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"inputshape = tf.keras.layers.InputLayer\\ndense = tf.keras.layers.Dense\\nconv2d = tf.keras.layers.Conv2D\\nmaxpool = tf.keras.layers.MaxPool2D\\nflatten = tf.keras.layers.Flatten\\nreshape = tf.keras.layers.Reshape\\nbatchNormalization = tf.keras.layers.BatchNormalization\\nleakyrelu = tf.keras.layers.LeakyReLU()\\n\\ndef generator():\\n  model = tf.keras.Sequential(name='Generator')\\n\\n  model.add(inputshape((latent_size, )))\\n  model.add(dense(128, activation=leakyrelu))\\n  model.add(batchNormalization())\\n    \\n  model.add(dense(512, activation=leakyrelu))\\n  model.add(batchNormalization())\\n      \\n  model.add(dense(2048, activation=leakyrelu))\\n  model.add(batchNormalization())\\n      \\n  model.add(dense(image_size**2, activation=leakyrelu))\\n  model.add(batchNormalization())\\n      \\n  model.add(reshape((image_size, image_size, 1)))\\n\\n  g_model = tf.keras.Model(\\n    inputs=model.get_layer(index=0),\\n    outputs=model.get_layer(index=-1),\\n  )\\n\\n  return g_model\\n\\ndef discriminator():\\n  model = tf.keras.Sequential(name='Discriminator')\\n\\n  model.add(inputshape((image_size, image_size, 1)))\\n  model.add()\\n  model.add(maxpool((2, 2)))\\n\\n  model.add()\\n  model.add(maxpool((2, 2)))\\n\\n  model.add()\\n  model.add(maxpool((2, 2)))\\n\\n  model.add()\\n  model.add(flatten())\\n\\n  model.add(dense(1024, activation=leakyrelu))\\n  model.add(dense(1))\\n\\n  d_model = tf.keras.Model(\\n    inputs=model.get_layer(index=0),\\n    outputs=model.get_layer(index=-1),\\n  )\\n\\n  return d_model\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQbWSuIGuDDL"
      },
      "source": [
        "# Generator\n",
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    dense = tf.keras.layers.Dense\n",
        "    batchNormalization = tf.keras.layers.BatchNormalization\n",
        "    reshape = tf.keras.layers.Reshape\n",
        "    leakyrelu = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "    self.sequence = list()\n",
        "    # Block 1\n",
        "    self.sequence.append(dense(128, activation=leakyrelu))\n",
        "    self.sequence.append(batchNormalization())\n",
        "    # Block 2\n",
        "    self.sequence.append(dense(512, activation=leakyrelu))\n",
        "    self.sequence.append(batchNormalization())\n",
        "    # Block 3\n",
        "    self.sequence.append(dense(2048, activation=leakyrelu))\n",
        "    self.sequence.append(batchNormalization())\n",
        "    # Block 4\n",
        "    self.sequence.append(dense(image_size**2, activation=leakyrelu))\n",
        "    self.sequence.append(batchNormalization())\n",
        "    \n",
        "    self.sequence.append(reshape([image_size, image_size, 1]))\n",
        "    \n",
        "  def call(self, z):\n",
        "    for layer in self.sequence:\n",
        "      z = layer(z)\n",
        "\n",
        "    return z"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1iL5aT4uB9K"
      },
      "source": [
        "# Discriminator\n",
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    conv2d = tf.keras.layers.Conv2D\n",
        "    maxpool = tf.keras.layers.MaxPool2D\n",
        "    dense = tf.keras.layers.Dense\n",
        "    flatten = tf.keras.layers.Flatten\n",
        "    leakyrelu = tf.keras.layers.LeakyReLU()\n",
        "\n",
        "    self.sequence = list()\n",
        "    # Block 1\n",
        "    self.sequence.append(conv2d(8, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))  \n",
        "    self.sequence.append(maxpool((2, 2)))                                      \n",
        "    # Block 2\n",
        "    self.sequence.append(conv2d(32, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))\n",
        "    self.sequence.append(maxpool((2, 2)))\n",
        "    # Block 3\n",
        "    self.sequence.append(conv2d(64, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))\n",
        "    self.sequence.append(maxpool((2, 2)))\n",
        "    # Block 4\n",
        "    self.sequence.append(conv2d(128, kernel_size=(3, 3), activation=leakyrelu, padding=\"same\"))\n",
        "    self.sequence.append(flatten())\n",
        "    # Block 5\n",
        "    self.sequence.append(dense(1024, activation=leakyrelu))\n",
        "    self.sequence.append(dense(1))\n",
        "    \n",
        "  def call(self, x):\n",
        "    for layer in self.sequence:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBVYtTCMTDSI"
      },
      "source": [
        "# Noise 생성부\n",
        "def make_Noise(batch_size, latent_size):\n",
        "  return tf.random.normal((batch_size, latent_size))\n",
        "\n",
        "# D_Model Loss 연산\n",
        "def discriminator_loss(loss, d_result_real, d_result_fake):\n",
        "    real_loss = loss(tf.ones_like(d_result_real), d_result_real)\n",
        "    fake_loss = loss(tf.zeros_like(d_result_fake), d_result_fake)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "# D_Model Optimizer\n",
        "def discriminator_optimizer(learning_rate):\n",
        "    return tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# G_Model Loss 연산\n",
        "def generator_loss(loss, d_result_fake):\n",
        "    return loss(tf.ones_like(d_result_fake), d_result_fake)\n",
        "\n",
        "# G_Model Optimizer\n",
        "def generator_optimizer(learning_rate):\n",
        "    return tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# G_Model에서 생성된 이미지 저장하는거\n",
        "def generate_and_save_images(g_model, epoch):\n",
        "  noises = make_Noise(32, 100)\n",
        "  g_predict = g_model(noises, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(6,6))\n",
        "\n",
        "  for i in range(g_predict.shape[0]):\n",
        "      plt.subplot(6, 6, i+1)\n",
        "      plt.imshow(g_predict[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "\n",
        "@tf.function\n",
        "def train_step(g_model, d_model, loss_function, g_optimizer, d_optimizer, batch_size, latent_size, learning_rate, real_images):\n",
        "  noises = make_Noise(batch_size, latent_size)\n",
        "\n",
        "  with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "    g_images = g_model(noises)\n",
        "\n",
        "    d_result_real = d_model(real_images, training=True)\n",
        "    d_result_fake = d_model(g_images, training=True)\n",
        "\n",
        "    g_loss = generator_loss(loss_function, d_result_fake)\n",
        "    d_loss = discriminator_loss(loss_function, d_result_real, d_result_fake)\n",
        "\n",
        "  g_gradients = g_tape.gradient(g_loss, g_model.trainable_variables)\n",
        "  d_gradients = d_tape.gradient(d_loss, d_model.trainable_variables)\n",
        "\n",
        "  g_optimizer.apply_gradients(zip(g_gradients, g_model.trainable_variables))\n",
        "  d_optimizer.apply_gradients(zip(d_gradients, d_model.trainable_variables))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "BpiYT3ovWM_5",
        "outputId": "fdf87855-f9d6-42ac-fdd0-fc346ba40c0e"
      },
      "source": [
        "g_model = Generator()\n",
        "d_model = Discriminator()\n",
        "\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "g_optimizer = generator_optimizer(learning_rate)\n",
        "d_optimizer = discriminator_optimizer(learning_rate)\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  start = time.time()\n",
        "\n",
        "  for real_images in train_dataset:\n",
        "    train_step(g_model, d_model, loss_function, g_optimizer, d_optimizer, batch_size, latent_size, learning_rate, real_images)\n",
        "\n",
        "  if (epoch % 100) == 0:\n",
        "    generate_and_save_images(g_model, epoch)\n",
        "\n",
        "  # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\n",
        "  # print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 4576/10000 [33:08<39:16,  2.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7722896ebe9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mreal_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m       dataset = _OptimizeDataset(dataset, graph_rewrites.enabled,\n\u001b[1;32m    462\u001b[0m                                  \u001b[0mgraph_rewrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                                  graph_rewrites.default, graph_rewrite_configs)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;31m# (4) Apply stats aggregator options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, optimization_configs)\u001b[0m\n\u001b[1;32m   4894\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4895\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4896\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4898\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset_v2\u001b[0;34m(input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0moptimizations_disabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   4175\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}